# -*- coding: utf-8 -*-
"""01_Jmcqgh_EDA_Carga_de_datos_y_visual.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CmFZW5Y3mVSWndFU6LuSJ8f3QLU1wwIJ

# MODELO PREDICTIVO PARA EL DESARROLLO DE PROGRAMAS CON ENFOQUE DIFERENCIAL PARA LAS MUJERES DEL DEPARTAMENTO DE CALDAS

El objetivo principal es Desarrollar un modelo no supervisado, con inteligencia artificial, que le permita a los actores sociales y territoriales del departamento de Caldas, identificar patrones cr铆ticos en salud mental que promuevan programas de promoci贸n y prevenci贸n con enfoque de g茅nero.




Pendiente agregar composici贸n de la base de datos, variables descritas!!!

Gedny Libeth Hern谩ndez Montoya
Cristina Quintero Escobar
Silvia Juliana Macias Parra
"""

# @title Conexi贸n con la base de datos desde una URL

#@markdown - Forma 1 de descarga de archivos desde Google Drive con wget
# Forma de descarga de archivos desde Google Drive con wget
# Se requiere el uso de cookies para la autenticaci贸n
# Se debe obtener el ID del archivo de Google Drive para poder descargarlo
# Se usa el comando wget para descargar el archivo, gestionando las cookies
# param url: URL de la base de datos
# param nombre_archivo: Nombre del archivo a guardar

# URL = 'https://drive.google.com/file/d/1ugDytz2MBJhIAQUVG6wzPZa6HOpboS32/view?usp=sharing'
# Definir el ID del archivo de Google Drive
FILEID = '1ugDytz2MBJhIAQUVG6wzPZa6HOpboS32'
nombre_archivo = 'RIPS_consulta_salud_mental.csv'

# Descargar el archivo usando wget, gestionando las cookies
# Se usa --load-cookies para cargar las cookies guardadas temporalmente en /tmp/cookies.txt
# Se hace una primera solicitud con wget para obtener el token de confirmaci贸n que Google Drive env铆a cuando el archivo es grande o muy accedido
# El token se extrae usando sed (una utilidad para filtrar y procesar texto), que busca el patr贸n "confirm=" y captura el valor del token
# Una vez obtenido el token de confirmaci贸n, se realiza la descarga completa del archivo
# --no-check-certificate es usado para evitar errores de SSL (certificados) en entornos no seguros
# El archivo se guarda como "Spotify_Most_Streamed_Songs.csv" . indicado con el nombre del archivo en la variable "nombre_archivo"
# Finalmente, se eliminan las cookies temporales almacenadas en /tmp/cookies.txt

# con esta instrucci贸n se descarga el archivo desde Google Drive
!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILEID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id="$FILEID -O $nombre_archivo && rm -rf /tmp/cookies.txt

"""# Desglose:

- FILEID = '1M8fB-SdAEZZ98_jEaM5ifyhlh9np3D99': El ID del archivo de Google Drive que se desea descargar.
- Primera llamada wget:
Obtiene un token de confirmaci贸n cuando Google Drive requiere verificaci贸n extra para archivos grandes o descargados muchas veces.\
Usa sed para buscar y extraer el token de la respuesta de Google Drive.
- Segunda llamada wget:
Con el token, se realiza la descarga final del archivo.\
rm -rf /tmp/cookies.txt: Limpia las cookies temporales despu茅s de completar la descarga.
"""

# Commented out IPython magic to ensure Python compatibility.
# @title Instalar librerias a usar
# %pip install sweetviz
# %pip install missingno

# @title Carga de librerias
# Bibliotecas utilizadas
# ----------------------------------------------------------------------------
# pandas (pd): librer铆a para el an谩lisis de datos
# matplotlib (plt): librer铆a para crear gr谩ficos
# seaborn (sns): librer铆a para visualizar datos de manera est茅tica
# Sweetviz (sw): Una biblioteca de an谩lisis y visualizaci贸n de datos en Python.
# files : Importa la funci贸n para subir y descargar archivos en Google Colab.
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sweetviz as sw
from google.colab import files
import numpy as np
import missingno as msno

# @title Cargamos las funciones relacionadas
def analizar_columnas(df):
  """
  Analiza las columnas de un DataFrame y devuelve informaci贸n estad铆stica.

  Par谩metros:
  df (DataFrame): El DataFrame a analizar.

  Retorno:
  Un DataFrame con la informaci贸n estad铆stica de cada columna.

  Ejemplo:
  df_analizado = analizar_columnas(df)
  print(df_analizado)
  """
  info = []
  for columna in df.columns:
    unicos = df[columna].nunique()
    nulos = df[columna].isnull().sum()
    total = len(df)
    porcentaje_nulos = (nulos / total) * 100
    info.append({
      'Columna': columna,
      'Cantidad de registros': total,
      'nicos': unicos,
      'Cantidad de nulos': nulos,
      'Porcentaje de nulos': porcentaje_nulos,
      'Registros relacionados con nulos': total - nulos
    })
  return pd.DataFrame(info)

"""# Secci贸n 1

### Paso 1: Cargar el archivo CSV
"""

# @title Cargar el archivo CSV de canciones de Spotify
# Ubicaci贸n en donde se almacena en drive
df_analizado = pd.read_csv(nombre_archivo, encoding="ISO-8859-1", delimiter=';')  # Otra variante de Latin-1


# Mostrar las primeras filas del archivo para ver c贸mo est谩n organizados los datos
print("Primeras 5 filas de los datos de Spotify:")
df_analizado.head()

# @title Visualizaci贸n de composici贸n de la base de datos.
df_analizado.info()

"""### Paso 2: Explorar los datos

Vamos a verificar la estructura de los datos, la cantidad de filas y columnas, as铆 como cualquier dato vac铆o.
"""

# @title Verificar la forma de los datos
print(f"Forma de los datos (filas, columnas): {df_analizado.shape}")

# @title Verificar los tipos de datos de cada columna
print("\nTipos de datos por columna:")
print(df_analizado.dtypes)

# Replace NaN values with 0 in 'FEMENINO' and 'MASCULINO' columns
df_analizado['FEMENINO'].fillna(0, inplace=True)
df_analizado['MASCULINO'].fillna(0, inplace=True)

# Assuming 'Ubicaci贸n' is the column you want to split
# and the separator is '-'

def split_location(df):
    """
    Splits the 'Ubicaci贸n' column into 'C贸digo' and 'Ubicaci贸n' based on '-'.

    Args:
        df: The input DataFrame.

    Returns:
        The DataFrame with added 'C贸digo' and 'Ubicaci贸n' columns.
            Returns the original DataFrame if 'Ubicaci贸n' column is not found or if splitting fails.
    """
    try:
      if 'Ubicaci贸n' not in df.columns:
          print("Error: 'Ubicaci贸n' column not found in the DataFrame.")
          return df

      df[['C贸digo', 'Nueva_Ubicaci贸n']] = df['Ubicaci贸n'].str.split('-', n=1, expand=True)
      return df
    except Exception as e:
        print(f"An error occurred during location splitting: {e}")
        return df

# Example usage (assuming df_analizado is your DataFrame):
df_analizado = split_location(df_analizado)
df_analizado.drop(columns=['Ubicaci贸n'], inplace=True)
# Display the updated DataFrame to verify the changes
df_analizado.head()

import pandas as pd
import re

# Funci贸n para clasificar cada entrada
def clasificar_atencion(fila):
    # Extraer c贸digo y descripci贸n
    partes = fila.split(' - ', 1)
    codigo = partes[0].strip()

    # Limpiar descripci贸n si existe
    descripcion = partes[1].replace('\x86', '').strip() if len(partes) > 1 else ''

    # Determinar tipo de entrada
    if codigo.isdigit():
        return {
            'Categoria_Principal': 'Geografia',
            'Subcategoria': descripcion,
            'Codigo': codigo,
            'Descripcion': descripcion
        }

    # Clasificaci贸n CIE-10
    clasificacion = {
        'Categoria_Principal': 'Otros trastornos',
        'Subcategoria': descripcion,
        'Codigo': codigo,
        'Descripcion': descripcion
    }

    # Extraer parte num茅rica del c贸digo
    match = re.match(r'F(\d{2})', codigo)
    if match:
        codigo_num = int(match.group(1))

        if 0 <= codigo_num <= 9:
            clasificacion['Categoria_Principal'] = 'Trastornos mentales org谩nicos'#'#F00-F09)'
        elif 10 <= codigo_num <= 19:
            clasificacion['Categoria_Principal'] = 'Trastornos por sustancias'#'#F10-F19)'
        elif 20 <= codigo_num <= 29:
            clasificacion['Categoria_Principal'] = 'Esquizofrenia'#F20-F29)'
        elif 30 <= codigo_num <= 39:
            clasificacion['Categoria_Principal'] = 'Trastornos del humor'#F30-F39)'
        elif 40 <= codigo_num <= 48:
            clasificacion['Categoria_Principal'] = 'Trastornos neur贸ticos'#F40-F48)'
        elif 50 <= codigo_num <= 59:
            clasificacion['Categoria_Principal'] = 'S铆ndromes comportamentales'#F50-F59)'
        elif 60 <= codigo_num <= 69:
            clasificacion['Categoria_Principal'] = 'Trastornos personalidad'#F60-F69)'
        elif 70 <= codigo_num <= 79:
            clasificacion['Categoria_Principal'] = 'Retraso mental'#F70-F79)'
        elif 80 <= codigo_num <= 89:
            clasificacion['Categoria_Principal'] = 'Trastornos desarrollo'#F80-F89)'
        elif 90 <= codigo_num <= 98:
            clasificacion['Categoria_Principal'] = 'Trastornos comportamiento infantil'#F90-F98)'
        elif codigo_num == 99:
            clasificacion['Categoria_Principal'] = 'Trastorno mental no especificado'#F99)'

    return clasificacion

# Aplicar la funci贸n al DataFrame
df_clasificado = df_analizado['Atenci贸n'].apply(
    lambda x: pd.Series(clasificar_atencion(x))
)

# Unir con el DataFrame original
df_final = pd.concat([df_analizado, df_clasificado], axis=1)

# Ejemplo de resultado
df_final[['Atenci贸n', 'Categoria_Principal', 'Subcategoria']].head()

# @title Verificaci贸n de los campos con relaci贸n a la composici贸n de la base de dato almacenada en formato CSV
df_analizado_ = analizar_columnas(df_final)
df_analizado_

df_final

# Visualizar valores nulos con un gr谩fico de barras
msno.bar(df_final)

# Visualizar matriz de valores faltantes
msno.matrix(df_final)

"""# Secci贸n 2"""

# @title eliminar colmnas con una cantidad menor a lo relacionado

# Calculate the percentage of missing values in each column
missing_percentage = df_final.isnull().sum() / len(df_final) * 100

# Identify columns with more than 20% missing values
columns_to_drop = missing_percentage[missing_percentage >= 20].index

# Drop the identified columns from the DataFrame
df_final = df_final.drop(columns=columns_to_drop)

# Print the updated DataFrame shape
print(f"Shape of DataFrame after dropping columns: {df_final.shape}")

# Now df_final contains only the columns with less than or equal 60% of missing values

missing_percentage.plot()

"""### Paso 3: Graficar distribuciones

Ahora, vamos a generar gr谩ficos para visualizar algunas caracter铆sticas de las canciones, como la distribuci贸n de la popularidad, la cantidad de reproducciones, y cualquier otra m茅trica relevante.
"""

# @title An谩lisis exploratorio de datos (EDA)
#Configurar las opciones de visualizacion Sweetviz
sw.config_parser.read_string("""
                              [Output_Defaults]
                              html_layout = widescreen
                              html_scale = 1.0
                              notebook_layout = widescreen
                              notebook_scale = 0.9
                              notebook_width = 100%
                              notebook_height = 700
                              [Layout]
                              show_logo = 0
                              """)
nombre = 'df_final'
lista_columnas_estandarizar = df_final.columns.tolist()
for col in lista_columnas_estandarizar:
  if col in df_final.columns:
    # Try converting to numeric, if it fails, convert to string
    try:
      df_final[col] = pd.to_numeric(df_final[col], errors='raise')
    except (ValueError, TypeError):
      df_final[col] = df_final[col].astype(str)

advert_report = sw.analyze([df_final, nombre], pairwise_analysis='on') # Realizamos el an谩lisis de la base de datos inicial

#@markdown Guardar y mostrar reporte formato HTML
advert_report.show_html('EDA_df_final.html')

#@markdown Descarga y abre el reporte en una nueva pesta帽a del navegador
almacenar_archivo = input('Alamcenar archivo Si o No: ').title()
if almacenar_archivo == 'Si':
  files.download('/content/EDA_df_final.html')

#@title La funci贸n advert_report.show_notebook en Sweetviz muestra el reporte de an谩lisis exploratorio de datos (EDA) en un notebook Jupyter.

# Par谩metros:

# layout: ajusta el dise帽o ( 'widescreen' o 'full screen' )
# w y h: ajusta el ancho y alto en p铆xeles
# scale: ajusta el tama帽o (0.9 = 90% de la resoluci贸n original)
advert_report.show_notebook(layout='widescreen', w=1500, h=500, scale=0.9)

#@title La funci贸n advert_report.show_notebook muestra el reporte de an谩lisis exploratorio de datos (EDA) en un notebook Jupyter, con el siguiente formato:

# layout='vertical': muestra los datos en una columna vertical
# w=1400 y h=500: ajusta el ancho y alto del reporte a 1400x500 p铆xeles
# scale=0.9: muestra el reporte a un 90% de su tama帽o original
advert_report.show_notebook(layout='vertical', w=1400, h=500, scale=0.9)

"""![](https://i.pinimg.com/originals/88/66/7e/88667eaf29f1bbf12d64abaaeae6caa2.gif)

# 3 Secci贸n
"""

df_final.Categoria_Principal.value_counts()

df_final.C贸digo.value_counts()

df_final.columns

# seleccionar_columnas = ['FEMENINO', 'MASCULINO', 'A帽o', 'C贸digo', 'Nueva_Ubicaci贸n',
#                         'Categoria_Principal', 'Subcategoria', 'Codigo', 'Descripcion']
seleccionar_columnas = ['FEMENINO', 'MASCULINO', 'A帽o', 'Nueva_Ubicaci贸n',
                        'Categoria_Principal', 'Subcategoria', 'Codigo']
df_final_columnas_seleccionadas = df_final[seleccionar_columnas]

# Commented out IPython magic to ensure Python compatibility.
# %pip install h2o
# %pip install rdt

# Importar librerias
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import h2o
from h2o.estimators.kmeans import H2OKMeansEstimator
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from rdt import HyperTransformer

df_final_columnas_seleccionadas.columns

df_final_columnas_seleccionadas.shape

#  Inicializar H2O
h2o.init()

# 1锔 Cargar el dataset
df = df_final_columnas_seleccionadas[['FEMENINO', 'MASCULINO', 'Categoria_Principal']].copy()

# 3锔 Aplicar transformaci贸n con RDT 
ht = HyperTransformer()
ht.detect_initial_config(df)
ht.fit(df)
df_transformed = ht.transform(df)
df_transformed = pd.DataFrame(df_transformed, columns=df.columns)

# 4锔 Convertir a H2OFrame 
df_h2o = h2o.H2OFrame(df_transformed)

# 5锔 Aplicar PCA y reducir a 2 dimensiones
pca = PCA(n_components=2)
df_pca = pca.fit_transform(df_transformed)

# 6锔 Definir valores de K a probar
k_values = range(2, 10)
sse_values = []
silhouette_values = []
cluster_results = {}

for k in k_values:
    #  Definir y entrenar K-Means en H2O
    kmeans = H2OKMeansEstimator(k=k)
    kmeans.train(x=df_h2o.columns, training_frame=df_h2o)

    #  Obtener SSE
    sse_values.append(kmeans.tot_withinss())

    #  Obtener etiquetas de clusters y calcular la silueta
    cluster_assignments = kmeans.predict(df_h2o)
    labels = h2o.as_list(cluster_assignments).values.flatten()

    # silhouette_score
    silhouette_avg = silhouette_score(df_pca, labels)  # Calcular silhouette
    silhouette_values.append(silhouette_avg)

    # Guardar resultados para visualizaci贸n
    cluster_results[k] = labels

#  Graficar M茅todo del Codo y Silhouette Score
fig, ax1 = plt.subplots(figsize=(8, 5))

ax1.plot(k_values, sse_values, marker='o', linestyle='--', color='b', label='SSE')
ax1.set_xlabel('N煤mero de Clusters (k)')
ax1.set_ylabel('Suma de Errores Cuadr谩ticos (SSE)', color='b')
ax1.tick_params(axis='y', labelcolor='b')

ax2 = ax1.twinx()
ax2.plot(k_values, silhouette_values, marker='s', linestyle='--', color='r', label='Silhouette Score')
ax2.set_ylabel('Puntuaci贸n de Silueta', color='r')
ax2.tick_params(axis='y', labelcolor='r')

plt.title('M茅todo del Codo y Silhouette Score para K-Means')
plt.show()

#  Visualizaci贸n de los Clusters en 2D (PCA)
best_k = max(cluster_results, key=lambda k: silhouette_values[k_values.index(k)])  # k con mejor silhouette
best_labels = cluster_results[best_k]

plt.figure(figsize=(7, 5))
plt.scatter(df_pca[:, 0], df_pca[:, 1], c=best_labels, cmap='rainbow', alpha=0.7)
plt.title(f'Visualizaci贸n de Clusters con PCA (k={best_k})')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.colorbar(label='Cluster')
plt.show()

# prompt: necesito el codigo de como quedan en orden las ceracter铆sticas relevantes luego de aplicar lo siguiente:
# pca = PCA(n_components=2)
# df_pca = pca.fit_transform(df_transformed)

# The order of features after PCA is not preserved in the original feature space.
# PCA creates new components (principal components) that are linear combinations of the original features.
# df_pca[:, 0] represents the first principal component, and df_pca[:, 1] the second.
# These components do not directly correspond to the original columns in df_transformed.

# To understand the contribution of original features to these new components, you can examine:

# 1. pca.components_:  This attribute of the fitted PCA object gives the loadings of each original feature on each principal component.
#    Each row corresponds to a principal component, and each column corresponds to an original feature. The value represents the weight of that feature in that component.
#    Higher absolute values indicate a larger contribution.

# Example:
print(pca.components_)

# You can then create a DataFrame for better readability
loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=df_transformed.columns)
print(loadings)

# 2. pca.explained_variance_ratio_: This shows the variance explained by each principal component.
pca.explained_variance_ratio_

# This helps you understand how much information each PC captures.


# In summary:  The order of features in df_pca does *not* reflect the original feature order.
# The loadings (pca.components_) provide the mapping between original features and principal components.

# prompt: necesito que me ordenes las caracter铆sticas con mayor relevancia luego de realizar lo siguiente:
# pca = PCA(n_components=2)
# df_pca = pca.fit_transform(df_transformed)

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Assuming 'healthy_life_df_columnas_seleccionadas' is your DataFrame
# and it has been preprocessed as shown in the code.

# Select numerical columns for PCA
df = df_final_columnas_seleccionadas.copy()

# 3锔 Aplicar transformaci贸n con RDT 
ht = HyperTransformer()
ht.detect_initial_config(df)
ht.fit(df)
df_transformed = ht.transform(df)
df_transformed = pd.DataFrame(df_transformed, columns=df.columns)

# Apply PCA
pca = PCA(n_components=2)
df_pca = pca.fit_transform(df_transformed)

# Get explained variance ratios
explained_variance_ratio = pca.explained_variance_ratio_

# Create a DataFrame with feature importance based on explained variance
feature_importance = pd.DataFrame({'feature': numeric_cols.columns,
                                    'importance': abs(pca.components_[0])}) # Use absolute value for ranking

# Sort by importance in descending order
feature_importance = feature_importance.sort_values('importance', ascending=False)


print("Feature Importance after PCA:")
print(feature_importance)

#Now you have feature_importance sorted, so you can access the features in order of importance:
most_important_features = feature_importance["feature"].to_list()
print("\nMost important features (ordered):", most_important_features)

# seleccionar las poblaciones
df_final_columnas_seleccionadas.shape

pd.DataFrame(best_labels).value_counts()

df_final_columnas_seleccionadas[best_labels==0]

df_final_columnas_seleccionadas[best_labels==1]

# @title An谩lisis exploratorio de datos (EDA)
#Configurar las opciones de visualizacion Sweetviz
sw.config_parser.read_string("""
                              [Output_Defaults]
                              html_layout = widescreen
                              html_scale = 1.0
                              notebook_layout = widescreen
                              notebook_scale = 0.9
                              notebook_width = 100%
                              notebook_height = 700
                              [Layout]
                              show_logo = 0
                              """)
nombre1 = 'grupo1'
nombre2 = 'grupo2'

advert_report = sw.compare([df_final_columnas_seleccionadas[best_labels==0], nombre1], [df_final_columnas_seleccionadas[best_labels==1], nombre2]) # Realizamos el an谩lisis de la base de datos inicial

#@markdown Guardar y mostrar reporte formato HTML
advert_report.show_html('EDA_df_final_columnas_seleccionadas.html')

#@markdown Descarga y abre el reporte en una nueva pesta帽a del navegador
almacenar_archivo = input('Alamcenar archivo Si o No: ').title()
if almacenar_archivo == 'Si':
  files.download('/content/EDA_df_final_columnas_seleccionadas.html')

#@title La funci贸n advert_report.show_notebook en Sweetviz muestra el reporte de an谩lisis exploratorio de datos (EDA) en un notebook Jupyter.

# Par谩metros:

# layout: ajusta el dise帽o ( 'widescreen' o 'full screen' )
# w y h: ajusta el ancho y alto en p铆xeles
# scale: ajusta el tama帽o (0.9 = 90% de la resoluci贸n original)
advert_report.show_notebook(layout='widescreen', w=1500, h=500, scale=0.9)